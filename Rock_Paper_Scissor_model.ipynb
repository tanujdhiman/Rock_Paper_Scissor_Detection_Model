{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Rock Paper Scisor model\n"
     ]
    }
   ],
   "source": [
    "print(\"This is Rock Paper Scisor model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and extract the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import re\n",
    "local_zip = r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\\rps.zip\"\n",
    "zip_ref = zipfile.ZipFile(local_zip)\n",
    "zip_ref.extractall(r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\")\n",
    "zip_ref.close()\n",
    "\n",
    "local_zip = r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\\rps-test-set.zip\"\n",
    "zip_ref = zipfile.ZipFile(local_zip)\n",
    "zip_ref.extractall(r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_dir = os.path.join(r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\\rps\\rock\")\n",
    "paper_dir = os.path.join(r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\\rps\\paper\")\n",
    "sci_dir = os.path.join(r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\\rps\\scissors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Images of rock paper and scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper images 840\n",
      "Rock images 840\n",
      "Scissors images 840\n"
     ]
    }
   ],
   "source": [
    "print(\"Paper images\", len(os.listdir(paper_dir)))\n",
    "print(\"Rock images\", len(os.listdir(rock_dir)))\n",
    "print(\"Scissors images\", len(os.listdir(sci_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\\rps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dir = r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\\rps-test-set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2520 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = training_datagen.flow_from_directory(\n",
    "                     training_dir, \n",
    "                    target_size = (150, 150),\n",
    "                    class_mode = \"categorical\",\n",
    "                    batch_size = 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 372 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                        validation_dir,\n",
    "                        target_size = (150, 150),\n",
    "                        class_mode = \"categorical\",\n",
    "                        batch_size = 126)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(64, (3,3), activation = \"relu\", input_shape = (150, 150, 3)),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    keras.layers.Conv2D(64, (3,3), activation = \"relu\"),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    keras.layers.Conv2D(128, (3,3), activation = \"relu\"),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    keras.layers.Conv2D(128, (3,3), activation = \"relu\"),\n",
    "    keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation = \"relu\"),\n",
    "    keras.layers.Dense(3, activation = \"softmax\")\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 1,866,819\n",
      "Trainable params: 1,866,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"RMSprop\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "20/20 [==============================] - 298s 15s/step - loss: 1.3988 - accuracy: 0.3472 - val_loss: 1.0846 - val_accuracy: 0.4113\n",
      "Epoch 2/25\n",
      "20/20 [==============================] - 167s 8s/step - loss: 1.1194 - accuracy: 0.3750 - val_loss: 1.0215 - val_accuracy: 0.4409\n",
      "Epoch 3/25\n",
      "20/20 [==============================] - 167s 8s/step - loss: 1.0394 - accuracy: 0.4948 - val_loss: 0.6421 - val_accuracy: 0.8387\n",
      "Epoch 4/25\n",
      "20/20 [==============================] - 183s 9s/step - loss: 0.8401 - accuracy: 0.5996 - val_loss: 0.4621 - val_accuracy: 1.0000\n",
      "Epoch 5/25\n",
      "20/20 [==============================] - 185s 9s/step - loss: 0.7068 - accuracy: 0.6750 - val_loss: 0.3964 - val_accuracy: 0.9758\n",
      "Epoch 6/25\n",
      "20/20 [==============================] - 167s 8s/step - loss: 0.6227 - accuracy: 0.7294 - val_loss: 0.2444 - val_accuracy: 0.8253\n",
      "Epoch 7/25\n",
      "20/20 [==============================] - 169s 8s/step - loss: 0.5060 - accuracy: 0.7861 - val_loss: 0.3164 - val_accuracy: 0.8280\n",
      "Epoch 8/25\n",
      "20/20 [==============================] - 169s 8s/step - loss: 0.4641 - accuracy: 0.8135 - val_loss: 0.4403 - val_accuracy: 0.8306\n",
      "Epoch 9/25\n",
      "20/20 [==============================] - 159s 8s/step - loss: 0.3636 - accuracy: 0.8591 - val_loss: 0.1405 - val_accuracy: 0.9892\n",
      "Epoch 10/25\n",
      "20/20 [==============================] - 162s 8s/step - loss: 0.2649 - accuracy: 0.9008 - val_loss: 0.2096 - val_accuracy: 0.8817\n",
      "Epoch 11/25\n",
      "20/20 [==============================] - 168s 8s/step - loss: 0.2392 - accuracy: 0.9135 - val_loss: 0.0423 - val_accuracy: 0.9892\n",
      "Epoch 12/25\n",
      "20/20 [==============================] - 238s 12s/step - loss: 0.2380 - accuracy: 0.9075 - val_loss: 0.1070 - val_accuracy: 0.9677\n",
      "Epoch 13/25\n",
      "20/20 [==============================] - 183s 9s/step - loss: 0.1308 - accuracy: 0.9556 - val_loss: 0.0698 - val_accuracy: 0.9731\n",
      "Epoch 14/25\n",
      "20/20 [==============================] - 160s 8s/step - loss: 0.2558 - accuracy: 0.9095 - val_loss: 0.1492 - val_accuracy: 0.9704\n",
      "Epoch 15/25\n",
      "20/20 [==============================] - 185s 9s/step - loss: 0.1118 - accuracy: 0.9631 - val_loss: 0.0545 - val_accuracy: 0.9785\n",
      "Epoch 16/25\n",
      "20/20 [==============================] - 161s 8s/step - loss: 0.2051 - accuracy: 0.9302 - val_loss: 0.0635 - val_accuracy: 0.9489\n",
      "Epoch 17/25\n",
      "20/20 [==============================] - 163s 8s/step - loss: 0.0885 - accuracy: 0.9675 - val_loss: 0.1275 - val_accuracy: 0.9435\n",
      "Epoch 18/25\n",
      "20/20 [==============================] - 178s 9s/step - loss: 0.1536 - accuracy: 0.9448 - val_loss: 0.0369 - val_accuracy: 0.9785\n",
      "Epoch 19/25\n",
      "20/20 [==============================] - 160s 8s/step - loss: 0.1129 - accuracy: 0.9671 - val_loss: 0.0765 - val_accuracy: 0.9812\n",
      "Epoch 20/25\n",
      "20/20 [==============================] - 168s 8s/step - loss: 0.1027 - accuracy: 0.9659 - val_loss: 0.0136 - val_accuracy: 0.9866\n",
      "Epoch 21/25\n",
      "20/20 [==============================] - 159s 8s/step - loss: 0.1169 - accuracy: 0.9536 - val_loss: 0.0709 - val_accuracy: 0.9651\n",
      "Epoch 22/25\n",
      "20/20 [==============================] - 292s 15s/step - loss: 0.0764 - accuracy: 0.9718 - val_loss: 0.6622 - val_accuracy: 0.8226\n",
      "Epoch 23/25\n",
      "20/20 [==============================] - 180s 9s/step - loss: 0.0697 - accuracy: 0.9746 - val_loss: 0.0301 - val_accuracy: 0.9919\n",
      "Epoch 24/25\n",
      "20/20 [==============================] - 174s 9s/step - loss: 0.0591 - accuracy: 0.9790 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "20/20 [==============================] - 173s 9s/step - loss: 0.0626 - accuracy: 0.9790 - val_loss: 0.0803 - val_accuracy: 0.9435\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, \n",
    "                             epochs = 25,\n",
    "                              steps_per_epoch=20,\n",
    "                             validation_data = validation_generator,\n",
    "                              verbose = 1, \n",
    "                              validation_steps=3)\n",
    "\n",
    "model.save(\"rps.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on new image [[ rock, paper, scissors]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "path = r\"C:\\Users\\Sunshine\\Downloads\\Compressed\\rock-paper-sci dataset\\rps-test-set\\scissors\\testscissors01-02.png\"\n",
    "img = image.load_img(path, target_size=(150, 150))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "classes = model.predict(images, batch_size=10)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
